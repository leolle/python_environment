{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33477771  0.17816966  0.55610314  0.02873565  0.25959148  0.06513102\n",
      "  0.81070633  0.13311718  0.17888095  0.81326147]\n",
      "[ 0.74518356  0.82497985  0.67907488  0.71692695  0.72768312  0.7255155\n",
      "  0.87746677  0.32288199  0.83628695  0.94226574]\n"
     ]
    }
   ],
   "source": [
    "rv_unif = stats.uniform.rvs(size=10)\n",
    "print (rv_unif)\n",
    "rv_beta = stats.beta.rvs(size=10, a=4, b=2)\n",
    "print (rv_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1:\n",
      "[ 0.78530896  0.92451326  0.60629615  0.74438526  0.68217998  0.83825149\n",
      "  0.19114278  0.96082945  0.63020022  0.91936102]\n",
      "method 2:\n",
      "[ 0.78530896  0.92451326  0.60629615  0.74438526  0.68217998  0.83825149\n",
      "  0.19114278  0.96082945  0.63020022  0.91936102]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "rv_beta = stats.beta.rvs(size=10, a=4, b=2)\n",
    "print (\"method 1:\")\n",
    "print (rv_beta)\n",
    "\n",
    "np.random.seed(0)\n",
    "beta = stats.beta(a=4, b=2)\n",
    "print (\"method 2:\")\n",
    "print (beta.rvs(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of data is: 0.219728329153\n",
      "median of data is: 0.274403126447\n",
      "standard deviation of data is: 1.99674109955\n"
     ]
    }
   ],
   "source": [
    "norm_dist = stats.norm(loc=0.5, scale=2)\n",
    "stats.norm??\n",
    "n = 200\n",
    "dat = norm_dist.rvs(size=n)\n",
    "print (\"mean of data is: \" + str(np.mean(dat)))\n",
    "print (\"median of data is: \" + str(np.median(dat)))\n",
    "print (\"standard deviation of data is: \" + str(np.std(dat)))\n",
    "\n",
    "### we can make it simple  as       \n",
    "    ##stats.norm.rvs(size = 200, loc = 0.5 , scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单样本假设检验问题，最为常见的解决方案是采用K-S检验（ Kolmogorov-Smirnov test）。单样本K-S检验的原假设是给定的数据来自和原假设分布相同的分布，在SciPy中提供了kstest函数，参数分别是数据、拟检验的分布名称和对应的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.mean(dat)\n",
    "sigma = np.std(dat)\n",
    "stat_val, p_val = stats.kstest(dat, 'norm', (mu, sigma))\n",
    "print ('KS-statistic D = %6.3f p-value = %6.4f' % (stat_val, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可进一步检验这组数据的均值是不是0。典型的方法是t检验（t-test），其中单样本的t检验函数为ttest_1samp："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stat_val, p_val = stats.ttest_1samp(dat, 0)\n",
    "print ('One-sample t-statistic D = %6.3f, p-value = %6.4f' % (stat_val, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时需要知道某数值在一个分布中的分位，或者给定了一个分布，求某分位上的数值。这可以通过cdf和ppf函数完成：\n",
    "另外对于一个给定的分布，可以用moment很方便的查看分布的矩信息，例如我们查看N(0,1)的六阶原点矩："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_dist = stats.gamma(a=2)\n",
    "print (\"quantiles of 2, 4 and 5:\")\n",
    "print (g_dist.cdf([2, 4, 5]))\n",
    "print (\"Values of 25%, 50% and 90%:\")\n",
    "print (g_dist.pdf([0.25, 0.5, 0.95]))\n",
    "\n",
    "stats.norm.moment(6, loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe函数提供对数据集的统计描述分析，包括数据样本大小，极值，均值，方差，偏度和峰度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_dist = stats.norm(loc=0, scale=1.8)\n",
    "dat = norm_dist.rvs(size=100)\n",
    "info = stats.describe(dat)\n",
    "print (\"Data size is: \" + str(info[0]))\n",
    "print (\"Minimum value is: \" + str(info[1][0]))\n",
    "print (\"Maximum value is: \" + str(info[1][1]))\n",
    "print (\"Arithmetic mean is: \" + str(info[2]))\n",
    "print (\"Unbiased variance is: \" + str(info[3]))\n",
    "print (\"Biased skewness is: \" + str(info[4]))\n",
    "print (\"Biased kurtosis is: \" + str(info[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以调用fit函数来得到对应分布参数的极大似然估计MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_dist = stats.norm(loc=0, scale=1.8)\n",
    "dat = norm_dist.rvs(size=100)\n",
    "mu, sigma = stats.norm.fit(dat)\n",
    "print (\"MLE of data mean:\" + str(mu))\n",
    "print (\"MLE of data standard deviation:\" + str(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pearsonr和spearmanr可以计算Pearson和Spearman相关系数，这两个相关系数度量了两组数据的相互线性关联程度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_dist = stats.norm()\n",
    "dat1 = norm_dist.rvs(size=100)\n",
    "exp_dist = stats.expon()\n",
    "dat2 = exp_dist.rvs(size=100)\n",
    "cor, pval = stats.pearsonr(dat1, dat2)\n",
    "print (\"Pearson correlation coefficient: \" + str(cor))\n",
    "cor, pval = stats.pearsonr(dat1, dat2)\n",
    "print (\"Spearman's rank correlation coefficient: \" + str(cor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分析金融数据中使用频繁的线性回归在SciPy中也有提供"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = stats.chi2.rvs(3, size=50)\n",
    "y = 2.5 + 1.2 * x + stats.norm.rvs(size=50, loc=0, scale=1.5)\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print (\"Slope of fitted model is:\" , slope)\n",
    "print (\"Intercept of fitted model is:\", intercept)\n",
    "print (\"R-squared:\", r_value**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化部分:以Rosenbrock函数为例\n",
    "首先定义Rosenbrock函数：f(\\mathbf{x}) = \\sum_{i=1}^{N-1} 100 (x_i - x_{i-1}^2)^2 + (1 - x_{i-1})^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nelder-Mead单纯形法\n",
    "单纯形法是运筹学中介绍的求解线性规划问题的通用方法，这里的Nelder-Mead单纯形法与其并不相同，只是用到单纯形的概念。设定起始点x0=(1.3,0.7,0.8,1.9,1.2)，并进行最小化的寻优。这里‘xtol’表示迭代收敛的容忍误差上界："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0 = np.array([0.5, 1.6, 1.1, 0.8, 1.2])\n",
    "res = opt.minimize(rosen, x_0, method='nelder-mead', options={'xtol': 1e-8, 'disp': True})\n",
    "print (\"Result of minimizing Rosenbrock function via Nelder-Mead Simplex algorithm:\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbrock函数的性质比较好，简单的优化方法就可以处理了，还可以在minimize中使用method='powell'来指定使用Powell's method。这两种简单的方法并不使用函数的梯度，在略微复杂的情形下收敛速度比较慢，下面让我们来看一下用到函数梯度进行寻优的方法。\n",
    "Broyden-Fletcher-Goldfarb-Shanno（BFGS）法用到了梯度信息，首先求一下Rosenbrock函数的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rosen_der(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return der\n",
    "##梯度信息的引入在minimize函数中通过参数jac指定：\n",
    "res = opt.minimize(rosen, x_0, method='BFGS', jac=rosen_der, options={'disp': True})\n",
    "print (\"Result of minimizing Rosenbrock function via Broyden-Fletcher-Goldfarb-Shanno algorithm:\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用到梯度的方法还有牛顿法，牛顿法是收敛速度最快的方法，其缺点在于要求Hessian矩阵（二阶导数矩阵）。牛顿法大致的思路是采用泰勒展开的二阶近似：其中H(x0)表示二阶导数矩阵。若Hessian矩阵是正定的，函数的局部最小值可以通过使上面的二次型的一阶导数等于0来获取，我们有：\n",
    "这里可使用共轭梯度近似Hessian矩阵的逆矩阵。下面给出Rosenbrock函数的Hessian矩阵元素通式\n",
    "为使用牛顿共轭梯度法，我们需要提供一个计算Hessian矩阵的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rosen_hess(x):\n",
    "    x = np.asarray(x)\n",
    "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "    diagonal = np.zeros_like(x)\n",
    "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "    diagonal[-1] = 200\n",
    "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "    H = H + np.diag(diagonal)\n",
    "    return H\n",
    "res = opt.minimize(rosen, x_0, method='Newton-CG', jac=rosen_der, hess=rosen_hess, options={'xtol': 1e-8, 'disp': True})\n",
    "print (\"Result of minimizing Rosenbrock function via Newton-Conjugate-Gradient algorithm (Hessian):\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一些大型的优化问题，Hessian矩阵将异常大，牛顿共轭梯度法用到的仅是Hessian矩阵和一个任意向量的乘积，为此，用户可以提供两个向量，一个是Hessian矩阵和一个任意向量p的乘积，另一个是向量p，这就减少了存储的开销。记向量p=(p1,…,pN−1)，可有\n",
    "\n",
    "[ \\mathbf{H(x)p} = \n",
    "[(1200x20−400x1+2)p0−400x0p1 ⋮ −400xi−1pi−1+(202+1200x2i−400xi+1)pi−400xipi+1 ⋮ −400xN−2pN−2+200pN−1]\n",
    " ]\n",
    "\n",
    "我们定义如下函数并使用牛顿共轭梯度方法寻优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rosen_hess_p(x, p):\n",
    "    x = np.asarray(x)\n",
    "    Hp = np.zeros_like(x)\n",
    "    Hp[0] = (1200*x[0]**2 - 400*x[1] + 2)*p[0] - 400*x[0]*p[1]\n",
    "    Hp[1:-1] = -400*x[:-2]*p[:-2]+(202+1200*x[1:-1]**2-400*x[2:])*p[1:-1] \\\n",
    "               -400*x[1:-1]*p[2:]\n",
    "    Hp[-1] = -400*x[-2]*p[-2] + 200*p[-1]\n",
    "    return Hp\n",
    "\n",
    "res = opt.minimize(rosen, x_0, method='Newton-CG', jac=rosen_der, hessp=rosen_hess_p, options={'xtol': 1e-8, 'disp': True})\n",
    "print (\"Result of minimizing Rosenbrock function via Newton-Conjugate-Gradient algorithm (Hessian times arbitrary vector):\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "无约束优化问题的一种标准形式为：\n",
    "[ \n",
    "minimizesubject tof(x)gi(x)≤0,i=1,…,mAx=b\n",
    "]\n",
    "其中g0,…,gm：ℝn→ℝ为ℝn空间上的二次可微的凸函数；A为p×n矩阵且秩rankA=p<n。\n",
    "\n",
    "我们考察如下一个例子：\n",
    "[ \n",
    "minimizesubject tof(x,y)=2xy+2x−x2−2y2x3−y=0y−1≥0\n",
    "]\n",
    "定义目标函数及其导数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(x, sign=1.0):\n",
    "    \"\"\" Objective function \"\"\"\n",
    "    return sign*(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2)\n",
    "\n",
    "def func_deriv(x, sign=1.0):\n",
    "    \"\"\" Derivative of objective function \"\"\"\n",
    "    dfdx0 = sign*(-2*x[0] + 2*x[1] + 2)\n",
    "    dfdx1 = sign*(2*x[0] - 4*x[1])\n",
    "    return np.array([ dfdx0, dfdx1 ])\n",
    "##其中sign表示求解最小或者最大值，我们进一步定义约束条件：\n",
    "cons = ({'type': 'eq',  'fun': lambda x: np.array([x[0]**3 - x[1]]), 'jac': lambda x: np.array([3.0*(x[0]**2.0), -1.0])},\n",
    "      {'type': 'ineq', 'fun': lambda x: np.array([x[1] - 1]), 'jac': lambda x: np.array([0.0, 1.0])})\n",
    "\n",
    "##最后我们使用SLSQP（Sequential Least SQuares Programming optimization algorithm）方法进行约束问题的求解（作为比较，同时列出了无约束优化的求解）：\n",
    "\n",
    "res = opt.minimize(func, [-1.0, 1.0], args=(-1.0,), jac=func_deriv, method='SLSQP', options={'disp': True})\n",
    "print \"Result of unconstrained optimization:\"\n",
    "print res\n",
    "res = opt.minimize(func, [-1.0, 1.0], args=(-1.0,), jac=func_deriv, constraints=cons, method='SLSQP', options={'disp': True})\n",
    "print \"Result of constrained optimization:\"\n",
    "print res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和统计部分一样，Python也有专门的优化扩展模块，CVXOPT（http://cvxopt.org ）专门用于处理凸优化问题，在约束优化问题上提供了更多的备选方法。CVXOPT是著名的凸优化教材convex optimization的作者之一，加州大学洛杉矶分校Lieven Vandenberghe教授的大作，是处理优化问题的利器。\n",
    "\n",
    "SciPy中的优化模块还有一些特殊定制的函数，专门处理能够转化为优化求解的一些问题，如方程求根、最小方差拟合等，可到SciPy优化部分的指引页面查看。\n",
    "\n",
    "####参考文献\n",
    "\n",
    "http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html\n",
    "http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html\n",
    "Boyd S. and Vandenberghe L. Convex optimization. Cambridge university press, 2004.\n",
    "\n",
    "https://uqer.io/community/share/54d83bb3f9f06c276f651a6e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
